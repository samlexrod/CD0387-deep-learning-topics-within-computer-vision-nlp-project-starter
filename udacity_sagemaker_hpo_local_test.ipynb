{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740fe431-f01d-44ae-9ea6-add181724216",
   "metadata": {},
   "source": [
    "# Save sample data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eea1e1d-c037-42d3-b6f3-0805f259f67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:06<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# List all objects helper internal function\n",
    "def list_all_objects(bucket, prefix):\n",
    "    # Create a paginator for list_objects_v2\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "    # Use the paginator to iterate through all pages\n",
    "    all_objects = []\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        if 'Contents' in page:\n",
    "            all_objects.extend(page['Contents'])\n",
    "\n",
    "    return all_objects\n",
    "\n",
    "bucket = 'udacity-deeplearning-project'\n",
    "prefix = 'data'\n",
    "sample_prefix = 'sample'\n",
    "\n",
    "all_files = list_all_objects(bucket, prefix)\n",
    "\n",
    "sample_files = [file_meta for file_meta in all_files if '001.Affenpinscher' in file_meta.get(\"Key\")]\n",
    "\n",
    "for file_meta in tqdm(sample_files):\n",
    "    key = file_meta.get(\"Key\")\n",
    "    \n",
    "    # Move the data from s3 to a different prefix\n",
    "    dirname = os.path.dirname(key)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    s3.download_file(bucket, key, key)\n",
    "    # s3.copy_object(\n",
    "    #     Bucket=bucket,\n",
    "    #     CopySource=f\"{bucket}/{key}\",\n",
    "    #     Key=f\"{sample_prefix}/{key}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201b549-bd17-499a-8a6f-05752ef5cb08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Estimator Locally Prior to Deployment to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87553aa0-2f98-4f0a-b3fb-e2dd11ec3053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:706584a72a49ef0971791982a66634be6cc0b8c4078e57f825c803483583a026\n",
      "REPOSITORY              TAG       IMAGE ID       CREATED                  SIZE\n",
      "udacity-sagemaker-hpo   latest    706584a72a49   Less than a second ago   3.92GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: udacity-sagemaker-hpo-2024-11-25-14-58-02-363\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.local.image:'Docker Compose' is not installed. Proceeding to check for 'docker-compose' CLI.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker Compose CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-bzqem:\n",
      "    command: train\n",
      "    container_name: 4463q2zj1d-algo-1-bzqem\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: udacity-sagemaker-hpo\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-bzqem\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmplqe68rhn/algo-1-bzqem/input:/opt/ml/input\n",
      "    - /tmp/tmplqe68rhn/algo-1-bzqem/output:/opt/ml/output\n",
      "    - /tmp/tmplqe68rhn/algo-1-bzqem/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmplqe68rhn/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /home/ec2-user/SageMaker/CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter/data/test:/opt/ml/input/data/train\n",
      "    - /home/ec2-user/SageMaker/CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter/data/valid:/opt/ml/input/data/test\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmplqe68rhn/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container 4463q2zj1d-algo-1-bzqem  Creating\n",
      " Container 4463q2zj1d-algo-1-bzqem  Created\n",
      "Attaching to 4463q2zj1d-algo-1-bzqem\n",
      "4463q2zj1d-algo-1-bzqem  | sed: can't read changehostname.c: No such file or directory\n",
      "4463q2zj1d-algo-1-bzqem  | \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kchangehostname.c: No such file or directory\n",
      "4463q2zj1d-algo-1-bzqem  | \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
      "4463q2zj1d-algo-1-bzqem  | compilation terminated.\n",
      "4463q2zj1d-algo-1-bzqem  | \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kchangehostname.o: No such file or directory\n",
      "4463q2zj1d-algo-1-bzqem  | ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,865 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,869 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,881 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,891 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,895 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,914 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,932 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:09,945 sagemaker-training-toolkit INFO     Invoking user script\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Training Env:\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | {\n",
      "4463q2zj1d-algo-1-bzqem  |     \"additional_framework_parameters\": {},\n",
      "4463q2zj1d-algo-1-bzqem  |     \"channel_input_dirs\": {\n",
      "4463q2zj1d-algo-1-bzqem  |         \"train\": \"/opt/ml/input/data/train\",\n",
      "4463q2zj1d-algo-1-bzqem  |         \"test\": \"/opt/ml/input/data/test\"\n",
      "4463q2zj1d-algo-1-bzqem  |     },\n",
      "4463q2zj1d-algo-1-bzqem  |     \"current_host\": \"algo-1-bzqem\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"hosts\": [\n",
      "4463q2zj1d-algo-1-bzqem  |         \"algo-1-bzqem\"\n",
      "4463q2zj1d-algo-1-bzqem  |     ],\n",
      "4463q2zj1d-algo-1-bzqem  |     \"hyperparameters\": {\n",
      "4463q2zj1d-algo-1-bzqem  |         \"num-classes\": 133,\n",
      "4463q2zj1d-algo-1-bzqem  |         \"batch-size\": 32,\n",
      "4463q2zj1d-algo-1-bzqem  |         \"lr\": 0.005070970373087015\n",
      "4463q2zj1d-algo-1-bzqem  |     },\n",
      "4463q2zj1d-algo-1-bzqem  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"input_data_config\": {\n",
      "4463q2zj1d-algo-1-bzqem  |         \"train\": {\n",
      "4463q2zj1d-algo-1-bzqem  |             \"TrainingInputMode\": \"File\"\n",
      "4463q2zj1d-algo-1-bzqem  |         },\n",
      "4463q2zj1d-algo-1-bzqem  |         \"test\": {\n",
      "4463q2zj1d-algo-1-bzqem  |             \"TrainingInputMode\": \"File\"\n",
      "4463q2zj1d-algo-1-bzqem  |         }\n",
      "4463q2zj1d-algo-1-bzqem  |     },\n",
      "4463q2zj1d-algo-1-bzqem  |     \"input_dir\": \"/opt/ml/input\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"is_master\": true,\n",
      "4463q2zj1d-algo-1-bzqem  |     \"job_name\": \"udacity-sagemaker-hpo-2024-11-25-14-58-02-363\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"log_level\": 20,\n",
      "4463q2zj1d-algo-1-bzqem  |     \"master_hostname\": \"algo-1-bzqem\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"model_dir\": \"/opt/ml/model\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"module_dir\": \"/opt/ml/code\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"module_name\": \"hpo\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"network_interface_name\": \"eth0\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"num_cpus\": 2,\n",
      "4463q2zj1d-algo-1-bzqem  |     \"num_gpus\": 0,\n",
      "4463q2zj1d-algo-1-bzqem  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"output_dir\": \"/opt/ml/output\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "4463q2zj1d-algo-1-bzqem  |     \"resource_config\": {\n",
      "4463q2zj1d-algo-1-bzqem  |         \"current_host\": \"algo-1-bzqem\",\n",
      "4463q2zj1d-algo-1-bzqem  |         \"hosts\": [\n",
      "4463q2zj1d-algo-1-bzqem  |             \"algo-1-bzqem\"\n",
      "4463q2zj1d-algo-1-bzqem  |         ]\n",
      "4463q2zj1d-algo-1-bzqem  |     },\n",
      "4463q2zj1d-algo-1-bzqem  |     \"user_entry_point\": \"hpo.py\"\n",
      "4463q2zj1d-algo-1-bzqem  | }\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Environment variables:\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | SM_HOSTS=[\"algo-1-bzqem\"]\n",
      "4463q2zj1d-algo-1-bzqem  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "4463q2zj1d-algo-1-bzqem  | SM_HPS={\"batch-size\":32,\"lr\":0.005070970373087015,\"num-classes\":133}\n",
      "4463q2zj1d-algo-1-bzqem  | SM_USER_ENTRY_POINT=hpo.py\n",
      "4463q2zj1d-algo-1-bzqem  | SM_FRAMEWORK_PARAMS={}\n",
      "4463q2zj1d-algo-1-bzqem  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-bzqem\",\"hosts\":[\"algo-1-bzqem\"]}\n",
      "4463q2zj1d-algo-1-bzqem  | SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "4463q2zj1d-algo-1-bzqem  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "4463q2zj1d-algo-1-bzqem  | SM_CHANNELS=[\"test\",\"train\"]\n",
      "4463q2zj1d-algo-1-bzqem  | SM_CURRENT_HOST=algo-1-bzqem\n",
      "4463q2zj1d-algo-1-bzqem  | SM_MODULE_NAME=hpo\n",
      "4463q2zj1d-algo-1-bzqem  | SM_LOG_LEVEL=20\n",
      "4463q2zj1d-algo-1-bzqem  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "4463q2zj1d-algo-1-bzqem  | SM_INPUT_DIR=/opt/ml/input\n",
      "4463q2zj1d-algo-1-bzqem  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "4463q2zj1d-algo-1-bzqem  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "4463q2zj1d-algo-1-bzqem  | SM_NUM_CPUS=2\n",
      "4463q2zj1d-algo-1-bzqem  | SM_NUM_GPUS=0\n",
      "4463q2zj1d-algo-1-bzqem  | SM_MODEL_DIR=/opt/ml/model\n",
      "4463q2zj1d-algo-1-bzqem  | SM_MODULE_DIR=/opt/ml/code\n",
      "4463q2zj1d-algo-1-bzqem  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-bzqem\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-bzqem\"],\"hyperparameters\":{\"batch-size\":32,\"lr\":0.005070970373087015,\"num-classes\":133},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"udacity-sagemaker-hpo-2024-11-25-14-58-02-363\",\"log_level\":20,\"master_hostname\":\"algo-1-bzqem\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"hpo\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-bzqem\",\"hosts\":[\"algo-1-bzqem\"]},\"user_entry_point\":\"hpo.py\"}\n",
      "4463q2zj1d-algo-1-bzqem  | SM_USER_ARGS=[\"--batch-size\",\"32\",\"--lr\",\"0.005070970373087015\",\"--num-classes\",\"133\"]\n",
      "4463q2zj1d-algo-1-bzqem  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "4463q2zj1d-algo-1-bzqem  | SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "4463q2zj1d-algo-1-bzqem  | SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "4463q2zj1d-algo-1-bzqem  | SM_HP_NUM-CLASSES=133\n",
      "4463q2zj1d-algo-1-bzqem  | SM_HP_BATCH-SIZE=32\n",
      "4463q2zj1d-algo-1-bzqem  | SM_HP_LR=0.005070970373087015\n",
      "4463q2zj1d-algo-1-bzqem  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Invoking script with the following command:\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | /opt/conda/bin/python3.6 hpo.py --batch-size 32 --lr 0.005070970373087015 --num-classes 133\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | -> Loading pretrained model...\n",
      "4463q2zj1d-algo-1-bzqem  | -> Freezing pre-trained model layers...\n",
      "4463q2zj1d-algo-1-bzqem  | -> Replacing pre-trained model classifier with 133 classes...\n",
      "4463q2zj1d-algo-1-bzqem  | ************************************************************\n",
      "4463q2zj1d-algo-1-bzqem  | -> USING LOCAL RUN...\n",
      "4463q2zj1d-algo-1-bzqem  | -> Using cross_entropy loss criterion...\n",
      "4463q2zj1d-algo-1-bzqem  | -> Using Adadelta optimizer...\n",
      "4463q2zj1d-algo-1-bzqem  | -> Creating custom data loaders...\n",
      "4463q2zj1d-algo-1-bzqem  | ************************************************************\n",
      "4463q2zj1d-algo-1-bzqem  | -> Starting model training...\n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 1 [0/8 (0%)]\tLoss: 4.451862\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.4967, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 2 [0/8 (0%)]\tLoss: 4.438112\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5036, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 3 [0/8 (0%)]\tLoss: 4.424025\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5093, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 4 [0/8 (0%)]\tLoss: 4.409708\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5138, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 5 [0/8 (0%)]\tLoss: 4.395213\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5173, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 6 [0/8 (0%)]\tLoss: 4.380572\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5198, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 7 [0/8 (0%)]\tLoss: 4.365804\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5216, Accuracy: 0/8 (0%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 8 [0/8 (0%)]\tLoss: 4.350925\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5228, Accuracy: 1/8 (12%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 9 [0/8 (0%)]\tLoss: 4.335946\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5238, Accuracy: 1/8 (12%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Train Epoch: 10 [0/8 (0%)]\tLoss: 4.320874\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | Test set: Average loss: 0.5243, Accuracy: 1/8 (12%)\n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | -> TorchScript model saved to /opt/ml/model/model.pth\n",
      "4463q2zj1d-algo-1-bzqem  | -> TorchScript model loaded successfully for verification.\n",
      "4463q2zj1d-algo-1-bzqem  | -> TorchScript model verification successful: Forward pass completed.\n",
      "4463q2zj1d-algo-1-bzqem  | ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "4463q2zj1d-algo-1-bzqem  | ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "4463q2zj1d-algo-1-bzqem  | ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "4463q2zj1d-algo-1-bzqem  | ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "4463q2zj1d-algo-1-bzqem  | Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 136MB/s]\n",
      "1it [00:00,  1.14it/s]m  | \n",
      "1it [00:00,  1.78it/s]m  | \n",
      "1it [00:00,  1.79it/s]m  | \n",
      "1it [00:00,  1.67it/s]m  | \n",
      "1it [00:00,  1.81it/s]m  | \n",
      "1it [00:00,  1.60it/s]m  | \n",
      "1it [00:00,  1.77it/s]m  | \n",
      "1it [00:00,  1.59it/s]m  | \n",
      "1it [00:00,  1.63it/s]m  | \n",
      "1it [00:00,  1.69it/s]m  | \n",
      "4463q2zj1d-algo-1-bzqem  | \n",
      "4463q2zj1d-algo-1-bzqem  | 2024-11-25 14:58:26,091 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "4463q2zj1d-algo-1-bzqem exited with code 0\n",
      "Aborting on container exit...\n",
      " Container 4463q2zj1d-algo-1-bzqem  Stopping\n",
      " Container 4463q2zj1d-algo-1-bzqem  Stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.local.image:Failed to delete: /tmp/tmplqe68rhn/algo-1-bzqem Please remove it manually.\n",
      "INFO:sagemaker.local.image:===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['sh', 'docker-build.sh'])\n",
    "\n",
    "role=get_execution_role()\n",
    "\n",
    "bucket = 'udacity-deeplearning-project'\n",
    "sample_prefix = 'sample'\n",
    "\n",
    "hyperparameters = {\n",
    "    'num-classes': 133,\n",
    "    'batch-size': 32,\n",
    "    'lr': 0.005070970373087015\n",
    "}\n",
    "\n",
    "bucket = 'udacity-deeplearning-project'\n",
    "s3_output_location = f\"s3://{bucket}/outputs\"\n",
    "\n",
    "estimator=Estimator(\n",
    "    image_uri='udacity-sagemaker-hpo',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='local',\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "model_inputs = {\n",
    "    \"train\": \"file://./data/test\",\n",
    "    \"test\": \"file://./data/valid\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=model_inputs, logs=True ,wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d3514-bc1c-40d5-a000-97292ccb17c8",
   "metadata": {},
   "source": [
    "## Testing Deployed Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f43502-7b17-4fa6-bddd-90a3271bb641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "push-container.sh: line 1: fg: no job control\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/1)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2B                                         0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/1) FINISHED                                 docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2B                                         0.0s\n",
      "\u001b[0m\u001b[?25hERROR: failed to solve: failed to read dockerfile: open Dockerfile: no such file or directory\n",
      "The push refers to repository [598308907998.dkr.ecr.us-east-1.amazonaws.com/udacity-sagemaker-hpo]\n",
      "\n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B36f7417d: Preparing \n",
      "\u001b[1B8d148110: Preparing \n",
      "\u001b[1B9d95c490: Preparing \n",
      "\u001b[1B95045e04: Preparing \n",
      "\u001b[1Be9708ca1: Preparing \n",
      "\u001b[1B8f6060c6: Preparing \n",
      "\u001b[1Ba3c12226: Preparing \n",
      "\u001b[1B62daa95e: Preparing \n",
      "\u001b[1B8fe1cb59: Preparing \n",
      "\u001b[1B061a5b0d: Preparing \n",
      "\u001b[1B3ff1bf08: Preparing \n",
      "\u001b[1Bfc0e8a35: Preparing \n",
      "\u001b[1B6a8aee3d: Preparing \n",
      "\u001b[1Be27443e4: Preparing \n",
      "\u001b[1Bd2b930fc: Preparing \n",
      "\u001b[1Bec0db89a: Preparing \n",
      "\u001b[1B49baa658: Preparing \n",
      "\u001b[2B49baa658: Layer already exists 1kB6A\u001b[2K\u001b[18A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2Klatest: digest: sha256:ecd3631bb1e4cd8805495bc1ad87bd02adb21a4111bd2d4e80306a3adc03f1f7 size: 4287\n"
     ]
    }
   ],
   "source": [
    "!sh push-container.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070d573d-0e9e-4862-a5c6-384483849ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/{sample_prefix}/data/train/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"test\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/{sample_prefix}/data/valid/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db355544-0e4d-424b-b8e7-246a93e08c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'num-classes': 133,\n",
    "    'batch-size': 32,\n",
    "    'lr': 0.005070970373087015\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ace2c8-ca83-4f3f-93a2-16d0126e1cdd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: udacity-sagemaker-hpo-2024-11-25-14-58-40-596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-25 14:58:42 Starting - Starting the training job...\n",
      "2024-11-25 14:58:57 Starting - Preparing the instances for training...\n",
      "2024-11-25 14:59:36 Downloading - Downloading the training image...\n",
      "2024-11-25 15:00:17 Training - Training image download completed. Training in progress..."
     ]
    }
   ],
   "source": [
    "estimator=Estimator(\n",
    "    image_uri='598308907998.dkr.ecr.us-east-1.amazonaws.com/udacity-sagemaker-hpo',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "estimator.fit(inputs=model_inputs, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7e61f-8be3-4f07-826e-06191f90414f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Get the latest training job name\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training Job Name: {training_job_name}\")\n",
    "\n",
    "# Get the model artifact location from the training job details\n",
    "response = sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_artifact = response[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(f\"Model Artifact Location: {model_artifact}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc42542-52b6-499e-b15e-38580aefd4dd",
   "metadata": {},
   "source": [
    "# Testing Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fc2e3bb-0705-4987-9333-71453dcb2584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact,  # Use the artifact location from the training job\n",
    "    role=estimator.role,       # Use the role from your estimator\n",
    "    entry_point=\"image/inference.py\",\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py3\",\n",
    "    image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.8.1-cpu-py36-ubuntu18.04\",  # PyTorch-Inference image\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69623137-c2b8-411e-b973-317986a06591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-598308907998/udacity-sagemaker-hpo-2024-11-24-20-20-00-150/output/model.tar.gz), script artifact (None), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-598308907998/pytorch-inference-2024-11-24-20-26-25-124/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2024-11-24-20-26-29-479\n"
     ]
    }
   ],
   "source": [
    "transformer = pytorch_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=\"s3://udacity-deeplearning-project/inference/\",  # S3 location for output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9375d00c-5d9e-4add-879a-f74929ec9a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2024-11-24-20-26-32-239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\u001b[34m2024-11-24 20:31:35,030 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:35,237 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.4.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 952 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:35,248 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:35,286 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:36,405 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:36,429 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:36,677 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:36,678 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:36,689 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,250 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,482 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,486 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:47.32533645629883|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,487 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.539794921875|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,487 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:15.3|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,488 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6570.9375|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,488 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:798.8828125|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,489 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:14.3|#Level:Host|#hostname:42a730c6c470,timestamp:1732480297\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,741 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,742 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]42\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,742 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,744 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,750 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,781 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,814 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,816 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]43\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,816 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,816 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,816 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:37,821 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,347 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 490\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,348 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:1930|#Level:Host|#hostname:42a730c6c470,timestamp:1732480298\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,361 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:49|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,436 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 532\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,437 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:2022|#Level:Host|#hostname:42a730c6c470,timestamp:1732480298\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:38,437 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:121|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,254 [INFO ] pool-1-thread-3 ACCESS_LOG - /169.254.255.130:55842 \"GET /ping HTTP/1.1\" 200 30\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,255 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,297 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:55856 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,297 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,606 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:181.33|#ModelName:model,Level:Model|#hostname:42a730c6c470,requestID:6f24d8c2-4e79-42ed-a5b7-8c7a7cf096ca,timestamp:1732480303\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,607 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 182\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,608 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:55870 \"POST /invocations HTTP/1.1\" 200 196\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,608 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,609 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[34m2024-11-24 20:31:43,609 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,254 [INFO ] pool-1-thread-3 ACCESS_LOG - /169.254.255.130:55842 \"GET /ping HTTP/1.1\" 200 30\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,255 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,297 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:55856 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,297 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,606 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:181.33|#ModelName:model,Level:Model|#hostname:42a730c6c470,requestID:6f24d8c2-4e79-42ed-a5b7-8c7a7cf096ca,timestamp:1732480303\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,607 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 182\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,608 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:55870 \"POST /invocations HTTP/1.1\" 200 196\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,608 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,609 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[35m2024-11-24 20:31:43,609 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:42a730c6c470,timestamp:null\u001b[0m\n",
      "\u001b[32m2024-11-24T20:31:43.312:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "Batch transform completed. Check the output in the specified S3 bucket.\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=\"s3://udacity-deeplearning-project/sample/data/batch/\",\n",
    "    content_type=\"application/x-image\",\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"Batch transform completed. Check the output in the specified S3 bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ce707-3d47-4a44-aa0e-784abd182860",
   "metadata": {
    "tags": []
   },
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
